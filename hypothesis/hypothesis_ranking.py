"""
Hypothesis ranking module.
Ranks generated hypotheses based on novelty, coherence, and evidence.
"""
import logging
import json
from pathlib import Path
from typing import List, Dict, Any
import numpy as np

from config.config import HYPOTHESIS_DIR
from utils.logging_config import logger


class HypothesisRanker:
    """Ranks hypotheses based on multiple criteria."""

    def __init__(self):
        """Initialize hypothesis ranker."""
        self.weights = {
            "novelty": 0.4,
            "coherence": 0.3,
            "evidence": 0.2,
            "testability": 0.1,
        }

    def rank_hypotheses(self, hypotheses: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Rank hypotheses by composite score.

        Args:
            hypotheses: List of hypothesis dictionaries

        Returns:
            Ranked list of hypotheses with scores
        """
        logger.info(f"Ranking {len(hypotheses)} hypotheses...")

        scored_hypotheses = []
        for hypothesis in hypotheses:
            score = self._calculate_composite_score(hypothesis)
            hypothesis["composite_score"] = score
            hypothesis["rank"] = 0  # Will be set after sorting
            scored_hypotheses.append(hypothesis)

        # Sort by composite score (descending)
        scored_hypotheses.sort(key=lambda x: x["composite_score"], reverse=True)

        # Assign ranks
        for i, hypothesis in enumerate(scored_hypotheses):
            hypothesis["rank"] = i + 1

        logger.info(f"Ranked {len(scored_hypotheses)} hypotheses")
        return scored_hypotheses

    def _calculate_composite_score(self, hypothesis: Dict[str, Any]) -> float:
        """
        Calculate composite score for a hypothesis.

        Args:
            hypothesis: Hypothesis dictionary

        Returns:
            Composite score (0-1)
        """
        # Novelty score (from agent or default)
        novelty = float(hypothesis.get("novelty_score", 0.5))

        # Coherence score (based on parameter consistency)
        coherence = self._calculate_coherence(hypothesis)

        # Evidence score (based on simulation context)
        evidence = self._calculate_evidence(hypothesis)

        # Testability score (based on hypothesis type and parameters)
        testability = self._calculate_testability(hypothesis)

        # Weighted composite score
        composite = (
            self.weights["novelty"] * novelty
            + self.weights["coherence"] * coherence
            + self.weights["evidence"] * evidence
            + self.weights["testability"] * testability
        )

        return float(np.clip(composite, 0.0, 1.0))

    def _calculate_coherence(self, hypothesis: Dict[str, Any]) -> float:
        """Calculate coherence score based on parameter consistency."""
        params = hypothesis.get("parameters", {})
        param1 = abs(params.get("param1", 0.0))
        param2 = abs(params.get("param2", 0.0))

        # Coherence is higher when parameters are in reasonable ranges
        coherence = 1.0 - min(1.0, (param1 + param2) / 2.0)
        return float(np.clip(coherence, 0.0, 1.0))

    def _calculate_evidence(self, hypothesis: Dict[str, Any]) -> float:
        """Calculate evidence score from simulation context."""
        context = hypothesis.get("simulation_context", {})
        success = context.get("success", False)
        num_steps = context.get("num_steps", 0)

        # Evidence is higher for successful simulations with more steps
        evidence = 0.5
        if success:
            evidence += 0.3
        if num_steps > 10:
            evidence += 0.2

        return float(np.clip(evidence, 0.0, 1.0))

    def _calculate_testability(self, hypothesis: Dict[str, Any]) -> float:
        """Calculate testability score."""
        hypothesis_type = hypothesis.get("type", 0)
        text = hypothesis.get("text", "")

        # Some hypothesis types are more testable
        type_scores = {0: 0.7, 1: 0.8, 2: 0.6, 3: 0.5}
        testability = type_scores.get(hypothesis_type, 0.5)

        # Bonus for specific, measurable language
        if any(word in text.lower() for word in ["may", "suggests", "indicates"]):
            testability += 0.1

        return float(np.clip(testability, 0.0, 1.0))


def rank_hypotheses_from_agent():
    """Rank hypotheses generated by the agent."""
    # This would typically load from agent output
    # For now, we'll create a sample ranking
    ranker = HypothesisRanker()

    # Sample hypotheses (in production, these come from agent.generate_hypotheses())
    sample_hypotheses = [
        {
            "type": 0,
            "text": "Structural hypothesis: The protein structure exhibits parameter_0.75 characteristics that may influence parameter_-0.25.",
            "parameters": {"param1": 0.75, "param2": -0.25},
            "novelty_score": 0.8,
            "simulation_context": {"success": True, "num_steps": 50},
        },
        {
            "type": 1,
            "text": "Functional hypothesis: The observed parameter_0.50 suggests a functional role in parameter_0.30 processes.",
            "parameters": {"param1": 0.50, "param2": 0.30},
            "novelty_score": 0.6,
            "simulation_context": {"success": True, "num_steps": 30},
        },
    ]

    ranked = ranker.rank_hypotheses(sample_hypotheses)

    # Save ranked hypotheses
    output_path = HYPOTHESIS_DIR / "ranked_hypotheses.json"
    with open(output_path, "w") as f:
        json.dump(ranked, f, indent=2)

    logger.info(f"Saved ranked hypotheses to {output_path}")
    return ranked


if __name__ == "__main__":
    rank_hypotheses_from_agent()

